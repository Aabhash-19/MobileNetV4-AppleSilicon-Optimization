{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43b775cb",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7333db89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b13b8d",
   "metadata": {},
   "source": [
    "### GPU Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dce43aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU ACCELERATION CHECK\n",
      "============================================================\n",
      "PyTorch Version: 2.8.0\n",
      "MPS Available: True\n",
      "MPS Built: True\n",
      "✅ USING MPS (APPLE SILICON GPU ACCELERATION)\n",
      "Selected Device: mps\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Setting environment variables for optimal performance\n",
    "os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.0'\n",
    "\n",
    "# GPU Checker Function\n",
    "def setup_device():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"GPU ACCELERATION CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    print(f\"MPS Available: {torch.backends.mps.is_available()}\")\n",
    "    print(f\"MPS Built: {torch.backends.mps.is_built()}\")\n",
    "    \n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(\"✅ USING MPS (APPLE SILICON GPU ACCELERATION)\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"✅ USING CUDA (NVIDIA GPU ACCELERATION)\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"⚠️  USING CPU ONLY (NO GPU ACCELERATION)\")\n",
    "    \n",
    "    print(f\"Selected Device: {device}\")\n",
    "    print(\"=\" * 60)\n",
    "    return device\n",
    "\n",
    "device = setup_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d27f06c",
   "metadata": {},
   "source": [
    "### Enhanced Components & Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc9352",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation Layer\"\"\"\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class StochasticDepth(nn.Module):\n",
    "    \"\"\"Stochastic Depth for Regularization\"\"\"\n",
    "    def __init__(self, drop_prob):\n",
    "        super(StochasticDepth, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.training or self.drop_prob == 0.:\n",
    "            return x\n",
    "        \n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x / keep_prob * random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee19772",
   "metadata": {},
   "source": [
    "### Layer Integration with Enhanced Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9982967a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedInvertedResidual(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride, expansion_ratio=6, \n",
    "                 use_se=True, use_swish=True):\n",
    "        super(EnhancedInvertedResidual, self).__init__()\n",
    "        hidden_dim = int(in_channels * expansion_ratio)\n",
    "        self.use_residual = stride == 1 and in_channels == out_channels\n",
    "        \n",
    "        layers = []\n",
    "        # Enhanced pointwise convolution\n",
    "        if expansion_ratio != 1:\n",
    "            layers.append(nn.Conv2d(in_channels, hidden_dim, 1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(hidden_dim))\n",
    "            layers.append(nn.SiLU() if use_swish else nn.ReLU6(inplace=True))\n",
    "        \n",
    "        # Depthwise convolution\n",
    "        layers.append(nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, \n",
    "                               groups=hidden_dim, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(hidden_dim))\n",
    "        layers.append(nn.SiLU() if use_swish else nn.ReLU6(inplace=True))\n",
    "        \n",
    "        # Squeeze-and-Excitation attention\n",
    "        if use_se:\n",
    "            layers.append(SELayer(hidden_dim, reduction=16))\n",
    "        \n",
    "        # Pointwise convolution\n",
    "        layers.append(nn.Conv2d(hidden_dim, out_channels, 1, bias=False))\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        self.stochastic_depth = StochasticDepth(0.1) if self.use_residual else None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.use_residual:\n",
    "            return x + self.stochastic_depth(self.conv(x))\n",
    "        else:\n",
    "            return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb13beb",
   "metadata": {},
   "source": [
    "### Excitation Layers with Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc1666b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SELayer(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation Layer\"\"\"\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class StochasticDepth(nn.Module):\n",
    "    \"\"\"Stochastic Depth for Regularization\"\"\"\n",
    "    def __init__(self, drop_prob):\n",
    "        super(StochasticDepth, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.training or self.drop_prob == 0.:\n",
    "            return x\n",
    "        \n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x / keep_prob * random_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516921d",
   "metadata": {},
   "source": [
    "### Building Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fc78786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMobileNetV4(nn.Module):\n",
    "    def __init__(self, num_classes=10, width_mult=1.2, dropout_rate=0.3, \n",
    "                 use_se=True, use_swish=True):\n",
    "        super(EnhancedMobileNetV4, self).__init__()\n",
    "        \n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "        \n",
    "        # Enhanced configuration\n",
    "        inverted_residual_setting = [\n",
    "            [1, 24, 1, 1],    # Increased channels\n",
    "            [6, 32, 2, 2],    # Increased channels\n",
    "            [6, 48, 3, 2],    # Increased channels\n",
    "            [6, 80, 4, 2],    # Increased channels\n",
    "            [6, 112, 3, 1],   # Increased channels\n",
    "            [6, 192, 3, 2],   # Increased channels\n",
    "            [6, 384, 1, 1],   # Increased channels\n",
    "        ]\n",
    "        \n",
    "        input_channel = int(input_channel * width_mult)\n",
    "        self.last_channel = int(last_channel * max(1.0, width_mult))\n",
    "        \n",
    "        # Enhanced initial layers\n",
    "        features = [nn.Sequential(\n",
    "            nn.Conv2d(3, input_channel, 3, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(input_channel),\n",
    "            nn.SiLU() if use_swish else nn.ReLU6(inplace=True)\n",
    "        )]\n",
    "        \n",
    "        # Build enhanced blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(EnhancedInvertedResidual(\n",
    "                    input_channel, output_channel, stride, t, use_se, use_swish\n",
    "                ))\n",
    "                input_channel = output_channel\n",
    "        \n",
    "        # Enhanced final layers\n",
    "        features.append(nn.Sequential(\n",
    "            nn.Conv2d(input_channel, self.last_channel, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.last_channel),\n",
    "            nn.SiLU() if use_swish else nn.ReLU6(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        ))\n",
    "        \n",
    "        self.features = nn.Sequential(*features)\n",
    "        \n",
    "        # Enhanced classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(self.last_channel, 512),\n",
    "            nn.SiLU() if use_swish else nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate/2),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Improved weight initialization\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f7255",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe53c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced training function\n",
    "def train_enhanced_model(model, train_loader, test_loader, device, epochs=20, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.05)  # AdamW\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_accuracies = []\n",
    "    \n",
    "    # Tracking time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch: {epoch+1:2d}/{epochs} | '\n",
    "                      f'Batch: {batch_idx:4d}/{len(train_loader)} | '\n",
    "                      f'Loss: {loss.item():.4f}')\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Enhanced evaluation with TTA\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                \n",
    "                # Test-time augmentation\n",
    "                outputs = model(data)\n",
    "                outputs_flip = model(torch.flip(data, [3]))\n",
    "                outputs = (outputs + outputs_flip) / 2.0\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        test_accuracies.append(accuracy)\n",
    "        \n",
    "        epoch_time = time.time() - epoch_start\n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Printing GPU memory info if using MPS\n",
    "        if torch.backends.mps.is_available():\n",
    "            memory_allocated = torch.mps.current_allocated_memory() / 1024**2\n",
    "            print(f'Epoch {epoch+1:2d}/{epochs} | '\n",
    "                  f'Loss: {avg_loss:.4f} | '\n",
    "                  f'Accuracy: {accuracy:.2f}% | '\n",
    "                  f'Time: {epoch_time:.1f}s | '\n",
    "                  f'GPU Mem: {memory_allocated:.1f}MB')\n",
    "        else:\n",
    "            print(f'Epoch {epoch+1:2d}/{epochs} | '\n",
    "                  f'Loss: {avg_loss:.4f} | '\n",
    "                  f'Accuracy: {accuracy:.2f}% | '\n",
    "                  f'Time: {epoch_time:.1f}s')\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f'Total training time: {total_time/60:.1f} minutes')\n",
    "    \n",
    "    return train_losses, test_accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f2c68b",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97014a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_enhanced_data(batch_size=64):\n",
    "    # Enhanced data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    \n",
    "    train_dataset = datasets.CIFAR10(root='./data', train=True, \n",
    "                                   download=True, transform=train_transform)\n",
    "    test_dataset = datasets.CIFAR10(root='./data', train=False, \n",
    "                                  download=True, transform=test_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, \n",
    "                             shuffle=True, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, \n",
    "                            shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae3b9ca",
   "metadata": {},
   "source": [
    "### Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "981e7a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "GPU ACCELERATION CHECK\n",
      "============================================================\n",
      "PyTorch Version: 2.8.0\n",
      "MPS Available: True\n",
      "MPS Built: True\n",
      "✅ USING MPS (APPLE SILICON GPU ACCELERATION)\n",
      "Selected Device: mps\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "TRAINING ENHANCED MOBILENETV4 MODEL\n",
      "============================================================\n",
      "Model Parameters: 6,413,864\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:684: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/20 | Batch:    0/782 | Loss: 2.5234\n",
      "Epoch:  1/20 | Batch:  100/782 | Loss: 2.3426\n",
      "Epoch:  1/20 | Batch:  200/782 | Loss: 1.7985\n",
      "Epoch:  1/20 | Batch:  300/782 | Loss: 1.9857\n",
      "Epoch:  1/20 | Batch:  400/782 | Loss: 1.8432\n",
      "Epoch:  1/20 | Batch:  500/782 | Loss: 2.0651\n",
      "Epoch:  1/20 | Batch:  600/782 | Loss: 1.9161\n",
      "Epoch:  1/20 | Batch:  700/782 | Loss: 1.8794\n",
      "Epoch  1/20 | Loss: 2.0309 | Accuracy: 39.30% | Time: 91.3s | GPU Mem: 186.2MB\n",
      "Epoch:  2/20 | Batch:    0/782 | Loss: 1.7923\n",
      "Epoch:  2/20 | Batch:  100/782 | Loss: 1.7336\n",
      "Epoch:  2/20 | Batch:  200/782 | Loss: 1.7918\n",
      "Epoch:  2/20 | Batch:  300/782 | Loss: 1.7717\n",
      "Epoch:  2/20 | Batch:  400/782 | Loss: 1.6729\n",
      "Epoch:  2/20 | Batch:  500/782 | Loss: 1.9002\n",
      "Epoch:  2/20 | Batch:  600/782 | Loss: 1.6235\n",
      "Epoch:  2/20 | Batch:  700/782 | Loss: 1.4110\n",
      "Epoch  2/20 | Loss: 1.7364 | Accuracy: 47.36% | Time: 78.2s | GPU Mem: 195.1MB\n",
      "Epoch:  3/20 | Batch:    0/782 | Loss: 1.5337\n",
      "Epoch:  3/20 | Batch:  100/782 | Loss: 1.6695\n",
      "Epoch:  3/20 | Batch:  200/782 | Loss: 1.6369\n",
      "Epoch:  3/20 | Batch:  300/782 | Loss: 1.6965\n",
      "Epoch:  3/20 | Batch:  400/782 | Loss: 1.6020\n",
      "Epoch:  3/20 | Batch:  500/782 | Loss: 1.5056\n",
      "Epoch:  3/20 | Batch:  600/782 | Loss: 1.5891\n",
      "Epoch:  3/20 | Batch:  700/782 | Loss: 1.6893\n",
      "Epoch  3/20 | Loss: 1.5941 | Accuracy: 56.30% | Time: 77.6s | GPU Mem: 195.1MB\n",
      "Epoch:  4/20 | Batch:    0/782 | Loss: 1.4558\n",
      "Epoch:  4/20 | Batch:  100/782 | Loss: 1.6044\n",
      "Epoch:  4/20 | Batch:  200/782 | Loss: 1.5556\n",
      "Epoch:  4/20 | Batch:  300/782 | Loss: 1.3885\n",
      "Epoch:  4/20 | Batch:  400/782 | Loss: 1.4171\n",
      "Epoch:  4/20 | Batch:  500/782 | Loss: 1.5904\n",
      "Epoch:  4/20 | Batch:  600/782 | Loss: 1.3533\n",
      "Epoch:  4/20 | Batch:  700/782 | Loss: 1.4792\n",
      "Epoch  4/20 | Loss: 1.4847 | Accuracy: 62.10% | Time: 77.9s | GPU Mem: 195.1MB\n",
      "Epoch:  5/20 | Batch:    0/782 | Loss: 1.3418\n",
      "Epoch:  5/20 | Batch:  100/782 | Loss: 1.3207\n",
      "Epoch:  5/20 | Batch:  200/782 | Loss: 1.3871\n",
      "Epoch:  5/20 | Batch:  300/782 | Loss: 1.3938\n",
      "Epoch:  5/20 | Batch:  400/782 | Loss: 1.5108\n",
      "Epoch:  5/20 | Batch:  500/782 | Loss: 1.4329\n",
      "Epoch:  5/20 | Batch:  600/782 | Loss: 1.2914\n",
      "Epoch:  5/20 | Batch:  700/782 | Loss: 1.2880\n",
      "Epoch  5/20 | Loss: 1.4032 | Accuracy: 64.76% | Time: 77.9s | GPU Mem: 195.1MB\n",
      "Epoch:  6/20 | Batch:    0/782 | Loss: 1.3511\n",
      "Epoch:  6/20 | Batch:  100/782 | Loss: 1.2874\n",
      "Epoch:  6/20 | Batch:  200/782 | Loss: 1.4713\n",
      "Epoch:  6/20 | Batch:  300/782 | Loss: 1.1402\n",
      "Epoch:  6/20 | Batch:  400/782 | Loss: 1.2801\n",
      "Epoch:  6/20 | Batch:  500/782 | Loss: 1.2211\n",
      "Epoch:  6/20 | Batch:  600/782 | Loss: 1.4742\n",
      "Epoch:  6/20 | Batch:  700/782 | Loss: 1.5734\n",
      "Epoch  6/20 | Loss: 1.3461 | Accuracy: 70.27% | Time: 78.6s | GPU Mem: 195.1MB\n",
      "Epoch:  7/20 | Batch:    0/782 | Loss: 1.3219\n",
      "Epoch:  7/20 | Batch:  100/782 | Loss: 1.4506\n",
      "Epoch:  7/20 | Batch:  200/782 | Loss: 1.2208\n",
      "Epoch:  7/20 | Batch:  300/782 | Loss: 1.5389\n",
      "Epoch:  7/20 | Batch:  400/782 | Loss: 1.5562\n",
      "Epoch:  7/20 | Batch:  500/782 | Loss: 1.3878\n",
      "Epoch:  7/20 | Batch:  600/782 | Loss: 1.2026\n",
      "Epoch:  7/20 | Batch:  700/782 | Loss: 1.3031\n",
      "Epoch  7/20 | Loss: 1.2971 | Accuracy: 71.89% | Time: 78.8s | GPU Mem: 195.1MB\n",
      "Epoch:  8/20 | Batch:    0/782 | Loss: 1.3114\n",
      "Epoch:  8/20 | Batch:  100/782 | Loss: 1.2142\n",
      "Epoch:  8/20 | Batch:  200/782 | Loss: 1.1527\n",
      "Epoch:  8/20 | Batch:  300/782 | Loss: 1.1619\n",
      "Epoch:  8/20 | Batch:  400/782 | Loss: 1.3961\n",
      "Epoch:  8/20 | Batch:  500/782 | Loss: 1.0941\n",
      "Epoch:  8/20 | Batch:  600/782 | Loss: 1.3421\n",
      "Epoch:  8/20 | Batch:  700/782 | Loss: 1.2426\n",
      "Epoch  8/20 | Loss: 1.2528 | Accuracy: 74.42% | Time: 79.2s | GPU Mem: 195.1MB\n",
      "Epoch:  9/20 | Batch:    0/782 | Loss: 1.3212\n",
      "Epoch:  9/20 | Batch:  100/782 | Loss: 1.3608\n",
      "Epoch:  9/20 | Batch:  200/782 | Loss: 1.1798\n",
      "Epoch:  9/20 | Batch:  300/782 | Loss: 1.3914\n",
      "Epoch:  9/20 | Batch:  400/782 | Loss: 1.4186\n",
      "Epoch:  9/20 | Batch:  500/782 | Loss: 1.3379\n",
      "Epoch:  9/20 | Batch:  600/782 | Loss: 1.1973\n",
      "Epoch:  9/20 | Batch:  700/782 | Loss: 1.0589\n",
      "Epoch  9/20 | Loss: 1.2125 | Accuracy: 75.80% | Time: 82.5s | GPU Mem: 195.1MB\n",
      "Epoch: 10/20 | Batch:    0/782 | Loss: 1.1435\n",
      "Epoch: 10/20 | Batch:  100/782 | Loss: 1.2861\n",
      "Epoch: 10/20 | Batch:  200/782 | Loss: 1.3659\n",
      "Epoch: 10/20 | Batch:  300/782 | Loss: 1.1489\n",
      "Epoch: 10/20 | Batch:  400/782 | Loss: 0.9907\n",
      "Epoch: 10/20 | Batch:  500/782 | Loss: 1.0309\n",
      "Epoch: 10/20 | Batch:  600/782 | Loss: 1.3425\n",
      "Epoch: 10/20 | Batch:  700/782 | Loss: 1.0292\n",
      "Epoch 10/20 | Loss: 1.1806 | Accuracy: 77.35% | Time: 83.8s | GPU Mem: 195.1MB\n",
      "Epoch: 11/20 | Batch:    0/782 | Loss: 1.1682\n",
      "Epoch: 11/20 | Batch:  100/782 | Loss: 1.0732\n",
      "Epoch: 11/20 | Batch:  200/782 | Loss: 1.0115\n",
      "Epoch: 11/20 | Batch:  300/782 | Loss: 1.2681\n",
      "Epoch: 11/20 | Batch:  400/782 | Loss: 1.2236\n",
      "Epoch: 11/20 | Batch:  500/782 | Loss: 1.1760\n",
      "Epoch: 11/20 | Batch:  600/782 | Loss: 0.9980\n",
      "Epoch: 11/20 | Batch:  700/782 | Loss: 1.0654\n",
      "Epoch 11/20 | Loss: 1.1481 | Accuracy: 78.10% | Time: 79.2s | GPU Mem: 195.1MB\n",
      "Epoch: 12/20 | Batch:    0/782 | Loss: 1.2366\n",
      "Epoch: 12/20 | Batch:  100/782 | Loss: 0.9556\n",
      "Epoch: 12/20 | Batch:  200/782 | Loss: 1.2104\n",
      "Epoch: 12/20 | Batch:  300/782 | Loss: 1.0548\n",
      "Epoch: 12/20 | Batch:  400/782 | Loss: 1.1671\n",
      "Epoch: 12/20 | Batch:  500/782 | Loss: 1.0141\n",
      "Epoch: 12/20 | Batch:  600/782 | Loss: 1.0003\n",
      "Epoch: 12/20 | Batch:  700/782 | Loss: 1.1351\n",
      "Epoch 12/20 | Loss: 1.1193 | Accuracy: 79.08% | Time: 81.7s | GPU Mem: 195.1MB\n",
      "Epoch: 13/20 | Batch:    0/782 | Loss: 1.0330\n",
      "Epoch: 13/20 | Batch:  100/782 | Loss: 1.0428\n",
      "Epoch: 13/20 | Batch:  200/782 | Loss: 1.2809\n",
      "Epoch: 13/20 | Batch:  300/782 | Loss: 0.9453\n",
      "Epoch: 13/20 | Batch:  400/782 | Loss: 1.0868\n",
      "Epoch: 13/20 | Batch:  500/782 | Loss: 1.0859\n",
      "Epoch: 13/20 | Batch:  600/782 | Loss: 1.1317\n",
      "Epoch: 13/20 | Batch:  700/782 | Loss: 1.0637\n",
      "Epoch 13/20 | Loss: 1.0958 | Accuracy: 79.59% | Time: 79.6s | GPU Mem: 195.1MB\n",
      "Epoch: 14/20 | Batch:    0/782 | Loss: 0.9935\n",
      "Epoch: 14/20 | Batch:  100/782 | Loss: 1.2242\n",
      "Epoch: 14/20 | Batch:  200/782 | Loss: 1.1101\n",
      "Epoch: 14/20 | Batch:  300/782 | Loss: 1.2113\n",
      "Epoch: 14/20 | Batch:  400/782 | Loss: 1.2505\n",
      "Epoch: 14/20 | Batch:  500/782 | Loss: 1.2498\n",
      "Epoch: 14/20 | Batch:  600/782 | Loss: 0.9691\n",
      "Epoch: 14/20 | Batch:  700/782 | Loss: 1.0587\n",
      "Epoch 14/20 | Loss: 1.0657 | Accuracy: 81.00% | Time: 81.3s | GPU Mem: 195.1MB\n",
      "Epoch: 15/20 | Batch:    0/782 | Loss: 1.0489\n",
      "Epoch: 15/20 | Batch:  100/782 | Loss: 1.0768\n",
      "Epoch: 15/20 | Batch:  200/782 | Loss: 1.1249\n",
      "Epoch: 15/20 | Batch:  300/782 | Loss: 1.0004\n",
      "Epoch: 15/20 | Batch:  400/782 | Loss: 1.1935\n",
      "Epoch: 15/20 | Batch:  500/782 | Loss: 0.9627\n",
      "Epoch: 15/20 | Batch:  600/782 | Loss: 1.1945\n",
      "Epoch: 15/20 | Batch:  700/782 | Loss: 1.1592\n",
      "Epoch 15/20 | Loss: 1.0430 | Accuracy: 81.69% | Time: 80.3s | GPU Mem: 195.1MB\n",
      "Epoch: 16/20 | Batch:    0/782 | Loss: 1.1558\n",
      "Epoch: 16/20 | Batch:  100/782 | Loss: 1.1190\n",
      "Epoch: 16/20 | Batch:  200/782 | Loss: 1.0995\n",
      "Epoch: 16/20 | Batch:  300/782 | Loss: 1.0751\n",
      "Epoch: 16/20 | Batch:  400/782 | Loss: 1.0246\n",
      "Epoch: 16/20 | Batch:  500/782 | Loss: 1.1784\n",
      "Epoch: 16/20 | Batch:  600/782 | Loss: 1.0145\n",
      "Epoch: 16/20 | Batch:  700/782 | Loss: 1.0867\n",
      "Epoch 16/20 | Loss: 1.0250 | Accuracy: 82.31% | Time: 81.2s | GPU Mem: 195.1MB\n",
      "Epoch: 17/20 | Batch:    0/782 | Loss: 1.0673\n",
      "Epoch: 17/20 | Batch:  100/782 | Loss: 0.8545\n",
      "Epoch: 17/20 | Batch:  200/782 | Loss: 0.9630\n",
      "Epoch: 17/20 | Batch:  300/782 | Loss: 1.0740\n",
      "Epoch: 17/20 | Batch:  400/782 | Loss: 0.8973\n",
      "Epoch: 17/20 | Batch:  500/782 | Loss: 0.9492\n",
      "Epoch: 17/20 | Batch:  600/782 | Loss: 1.0509\n",
      "Epoch: 17/20 | Batch:  700/782 | Loss: 0.8663\n",
      "Epoch 17/20 | Loss: 1.0120 | Accuracy: 82.62% | Time: 80.0s | GPU Mem: 195.1MB\n",
      "Epoch: 18/20 | Batch:    0/782 | Loss: 0.8904\n",
      "Epoch: 18/20 | Batch:  100/782 | Loss: 1.0030\n",
      "Epoch: 18/20 | Batch:  200/782 | Loss: 0.9235\n",
      "Epoch: 18/20 | Batch:  300/782 | Loss: 1.0200\n",
      "Epoch: 18/20 | Batch:  400/782 | Loss: 0.9392\n",
      "Epoch: 18/20 | Batch:  500/782 | Loss: 0.9815\n",
      "Epoch: 18/20 | Batch:  600/782 | Loss: 0.9500\n",
      "Epoch: 18/20 | Batch:  700/782 | Loss: 1.0381\n",
      "Epoch 18/20 | Loss: 1.0055 | Accuracy: 82.86% | Time: 84.1s | GPU Mem: 195.1MB\n",
      "Epoch: 19/20 | Batch:    0/782 | Loss: 1.0236\n",
      "Epoch: 19/20 | Batch:  100/782 | Loss: 0.9796\n",
      "Epoch: 19/20 | Batch:  200/782 | Loss: 0.7979\n",
      "Epoch: 19/20 | Batch:  300/782 | Loss: 0.9953\n",
      "Epoch: 19/20 | Batch:  400/782 | Loss: 1.1020\n",
      "Epoch: 19/20 | Batch:  500/782 | Loss: 0.8396\n",
      "Epoch: 19/20 | Batch:  600/782 | Loss: 1.0947\n",
      "Epoch: 19/20 | Batch:  700/782 | Loss: 0.9985\n",
      "Epoch 19/20 | Loss: 0.9912 | Accuracy: 83.17% | Time: 84.2s | GPU Mem: 195.1MB\n",
      "Epoch: 20/20 | Batch:    0/782 | Loss: 1.1473\n",
      "Epoch: 20/20 | Batch:  100/782 | Loss: 0.8973\n",
      "Epoch: 20/20 | Batch:  200/782 | Loss: 0.8399\n",
      "Epoch: 20/20 | Batch:  300/782 | Loss: 0.9156\n",
      "Epoch: 20/20 | Batch:  400/782 | Loss: 0.9275\n",
      "Epoch: 20/20 | Batch:  500/782 | Loss: 1.0309\n",
      "Epoch: 20/20 | Batch:  600/782 | Loss: 0.8582\n",
      "Epoch: 20/20 | Batch:  700/782 | Loss: 1.0642\n",
      "Epoch 20/20 | Loss: 0.9830 | Accuracy: 83.28% | Time: 80.4s | GPU Mem: 195.1MB\n",
      "Total training time: 27.0 minutes\n",
      "\n",
      "Final Test Accuracy (Enhanced): 83.28%\n",
      "Peak GPU Memory Usage: 1.25 GB\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Setting up device with GPU checker\n",
    "    device = setup_device()\n",
    "    \n",
    "    # Preparing enhanced data with same batch size\n",
    "    train_loader, test_loader = prepare_enhanced_data(batch_size=64)\n",
    "    \n",
    "    # Creating and train enhanced model\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"TRAINING ENHANCED MOBILENETV4 MODEL\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    enhanced_model = EnhancedMobileNetV4(num_classes=10).to(device)\n",
    "    \n",
    "    # Counting parameters\n",
    "    total_params = sum(p.numel() for p in enhanced_model.parameters())\n",
    "    print(f\"Model Parameters: {total_params:,}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    train_losses_enhanced, test_accuracies_enhanced = train_enhanced_model(\n",
    "        enhanced_model, train_loader, test_loader, device, epochs=20, lr=0.001\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFinal Test Accuracy (Enhanced): {test_accuracies_enhanced[-1]:.2f}%\")\n",
    "    \n",
    "    # Showing final GPU memory usage\n",
    "    if torch.backends.mps.is_available():\n",
    "        peak_memory = torch.mps.driver_allocated_memory() / 1024**3\n",
    "        print(f\"Peak GPU Memory Usage: {peak_memory:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
